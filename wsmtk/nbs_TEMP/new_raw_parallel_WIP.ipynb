{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "IndentationError",
     "evalue": "unexpected unindent (utils.py, line 129)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2910\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-ccd00f4ceddd>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from wsmtk.utils import *\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/data/code/wsmtk/wsmtk/utils.py\"\u001b[0;36m, line \u001b[0;32m129\u001b[0m\n\u001b[0;31m    def txx(x):\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected unindent\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import sys, os\n",
    "import time\n",
    "import datetime\n",
    "from subprocess import Popen, check_output\n",
    "import h5py\n",
    "from progress.bar import Bar\n",
    "from progress.spinner import Spinner\n",
    "from wsmtk.utils import *\n",
    "from contextlib import contextmanager, closing\n",
    "import warnings\n",
    "import itertools\n",
    "import bisect\n",
    "import gc\n",
    "import array\n",
    "import multiprocessing as mp\n",
    "import traceback\n",
    "try:\n",
    "    import gdal\n",
    "except ImportError:\n",
    "    from osgeo import gdal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MODISrawh5:\n",
    "    '''Class for raw MODIS data collected into HDF5 file, ready for smoothing.\n",
    "\n",
    "    For MOD/MYD 13 products, MOD and MYD are interleaved into a combined MXD.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,files,param=None,targetdir=os.getcwd(),compression='gzip',chunk=120*120):\n",
    "        \n",
    "        '''Create a MODISrawh5 class\n",
    "\n",
    "        Args:\n",
    "            files ([str]): A list of absolute paths to MODIS raw hdf files to be processed\n",
    "            param (str): VAM parameter to be processed (default VIM/LTD)\n",
    "            targetdir (str): Target directory for raw MODIS HDF5 file\n",
    "            compression (str): Compression method to be used (default = gzip)\n",
    "        '''\n",
    "\n",
    "        self.targetdir = targetdir\n",
    "        #self.resdict = dict(zip(['250m','500m','1km','0.05_Deg'],[x/112000 for x in [250,500,1000,5600]])) ## commented for original resolution\n",
    "        self.paramdict = dict(zip(['VIM', 'VEM', 'LTD', 'LTN'], ['NDVI', 'EVI', 'LST_Day', 'LST_Night']))\n",
    "        self.compression = compression\n",
    "        self.dts_regexp = re.compile(r'.+A(\\d{7}).+')\n",
    "        self.dates = [re.findall(self.dts_regexp,x)[0] for x in files]\n",
    "        self.chunk = chunk\n",
    "        self.files = [x for (y,x) in sorted(zip(self.dates,files))]\n",
    "        self.dates.sort()\n",
    "        self.nfiles = len(self.files)\n",
    "        self.ref_file = self.files[0]\n",
    "        self.ref_file_basename = os.path.basename(self.ref_file)\n",
    "\n",
    "        # class works only for M.D11 and M.D13 products\n",
    "        if not re.match(r'M.D13\\w\\d',self.ref_file_basename) and not re.match(r'M.D11\\w\\d',self.ref_file_basename):\n",
    "            \n",
    "            raise SystemExit(\"Processing only implemented for M*D11 or M*13 products!\")\n",
    "\n",
    "        # Patterns for string extraction\n",
    "        ppatt = re.compile(r'M\\w{6}')\n",
    "        vpatt = re.compile('.+\\.(\\d{3})\\..+')\n",
    "        tpatt = re.compile(r'h\\d+v\\d+')\n",
    "\n",
    "        # Open reference file\n",
    "        ref = gdal.Open(self.ref_file)\n",
    "\n",
    "        # When no parameter is selected, the default is VIM and LTD\n",
    "        if not param:\n",
    "            ref_sds = ref_sds = [x[0] for x in ref.GetSubDatasets() if self.paramdict['VIM'] in x[0] or self.paramdict['LTD'] in x[0]][0]\n",
    "            self.param = [key for key, value in self.paramdict.items() if value in ref_sds][0]\n",
    "            ref_sds = None\n",
    "        elif param in self.paramdict.keys():\n",
    "            self.param = param\n",
    "        else:\n",
    "            raise ValueError('Parameter string not recognized. Available parameters are %s.' % [x for x in self.paramdict.keys()])\n",
    "\n",
    "        ref = None\n",
    "\n",
    "        # check for MOD/MYD interleaving\n",
    "        if self.param is 'VIM' and any(['MOD' in os.path.basename(x) for x in files]) and any(['MYD' in os.path.basename(x) for x in files]):\n",
    "            self.product = [re.sub(r'M[O|Y]D','MXD',re.findall(ppatt,self.ref_file_basename)[0])]\n",
    "            self.temporalresolution = 8\n",
    "        else:\n",
    "            self.product = re.findall(ppatt,self.ref_file_basename)\n",
    "            self.temporalresolution = None\n",
    "\n",
    "        # Name of file to be created/updated\n",
    "        self.outname = '{}/{}/{}.{}.h5'.format(\n",
    "                                    self.targetdir,\n",
    "                                    self.param,\n",
    "                                    '.'.join(self.product + re.findall(tpatt,self.ref_file_basename) + [re.sub(vpatt,'\\\\1',self.ref_file_basename)]),\n",
    "                                    self.param)\n",
    "\n",
    "        self.exists = os.path.isfile(self.outname)\n",
    "        ref = None\n",
    "\n",
    "\n",
    "    def create(self):\n",
    "        '''Creates the HDF5 file.'''\n",
    "\n",
    "        print('\\nCreating file: {} ... '.format(self.outname), end='')\n",
    "\n",
    "        ref = gdal.Open(self.ref_file)\n",
    "        ref_sds = [x[0] for x in ref.GetSubDatasets() if self.paramdict[self.param] in x[0]][0]\n",
    "\n",
    "        rst = gdal.Open(ref_sds)\n",
    "        ref_sds = None\n",
    "\n",
    "        self.rows = rst.RasterYSize\n",
    "        self.cols = rst.RasterXSize\n",
    "        self.nodata_value = int(rst.GetMetadataItem('_FillValue'))\n",
    "\n",
    "        if re.match(r'M[O|Y]D13\\w\\d',self.ref_file_basename):\n",
    "            if not self.temporalresolution:\n",
    "                self.numberofdays = 16\n",
    "                self.temporalresolution = self.numberofdays\n",
    "\n",
    "            else:\n",
    "                self.numberofdays = 16\n",
    "\n",
    "        elif re.match(r'M[O|Y]D11\\w\\d',self.ref_file_basename):\n",
    "            self.numberofdays = 8\n",
    "            self.temporalresolution = self.numberofdays\n",
    "\n",
    "        # Read datatype\n",
    "        dt = rst.GetRasterBand(1).DataType\n",
    "\n",
    "        # Parse datatype - on error use default Int16\n",
    "        try:\n",
    "            self.datatype = dtype_GDNP(dt)\n",
    "        except IndexError:\n",
    "            print(\"\\n\\n Couldn't read data type from dataset. Using default Int16!\\n\")\n",
    "            self.datatype = (3,'int16')\n",
    "\n",
    "        self.chunks = (self.chunk,100)\n",
    "\n",
    "        trans = rst.GetGeoTransform()\n",
    "        prj = rst.GetProjection()\n",
    "\n",
    "        rst = None\n",
    "\n",
    "        # Create directory if necessary\n",
    "        if not os.path.exists(os.path.dirname(self.outname)):\n",
    "            os.makedirs(os.path.dirname(self.outname))\n",
    "\n",
    "        # Create HDF5 file\n",
    "        try:\n",
    "\n",
    "            with h5py.File(self.outname,'x',libver='latest') as h5f:\n",
    "                dset = h5f.create_dataset('data',shape=(self.rows*self.cols,self.nfiles),dtype=self.datatype[1],maxshape=(self.rows*self.cols,None),chunks=self.chunks,compression=self.compression,fillvalue=self.nodata_value)\n",
    "                h5f.create_dataset('dates',shape=(self.nfiles,),maxshape=(None,),dtype='S8',compression=self.compression)\n",
    "                dset.attrs['geotransform'] = trans\n",
    "                dset.attrs['projection'] = prj\n",
    "                dset.attrs['resolution'] = trans[1] # res ## commented for original resolution\n",
    "                dset.attrs['nodata'] = self.nodata_value\n",
    "                dset.attrs['numberofdays'] = self.numberofdays\n",
    "                dset.attrs['temporalresolution'] = self.temporalresolution\n",
    "                dset.attrs['rows'] = self.rows\n",
    "                dset.attrs['columns'] = self.cols\n",
    "\n",
    "            self.exists = True\n",
    "            print('done.\\n')\n",
    "\n",
    "        except:\n",
    "            print('\\n\\nError creating {}! Check input parameters (especially if compression/filter is available) and try again. Corrupt file will be removed now. \\n\\nError message: \\n'.format(self.outname))\n",
    "            os.remove(self.outname)\n",
    "            raise\n",
    "\n",
    "    def update(self):\n",
    "        '''Ingest raw data into MODIS raw HDF5 file.\n",
    "\n",
    "        When a new HDF5 file is created, uodate will also handle the first data ingest.\n",
    "        '''\n",
    "\n",
    "        print('Processing MODIS files ...\\n')\n",
    "\n",
    "        try:\n",
    "\n",
    "            print(\"READ METADATA\")\n",
    "            with h5py.File(self.outname,'r+',libver='latest') as h5f:\n",
    "                dset = h5f.get('data')\n",
    "                dts  = h5f.get('dates')\n",
    "                self.chunks = dset.chunks\n",
    "                self.rows = dset.attrs['rows'].item()\n",
    "                self.cols = dset.attrs['columns'].item()\n",
    "                self.nodata_value = dset.attrs['nodata'].item()\n",
    "                self.numberofdays = dset.attrs['numberofdays'].item()\n",
    "                self.temporalresolution = dset.attrs['temporalresolution'].item()\n",
    "                self.datatype = dtype_GDNP(dset.dtype.name)\n",
    "                #res  = dset.attrs['Resolution'] ## comment for original resolution\n",
    "                dset.attrs['processingtimestamp'] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "\n",
    "                # Insert new dates into existing date list\n",
    "                dates_combined = [x.decode() for x in dts[...] if len(x) > 0 and x.decode() not in self.dates]\n",
    "\n",
    "                #dates_combined = np.concatenate([dates_h5,self.dates])\n",
    "                \n",
    "                \n",
    "\n",
    "                n = len(dates_combined)\n",
    "\n",
    "                # if date list is bigger after insert, datasets need to be resized for additional data\n",
    "                if n > dts.shape[0]:\n",
    "                    dts.resize((n,))\n",
    "                    dset.resize((dset.shape[0],n))\n",
    "\n",
    "                sort_ix = np.argsort(dates_combined)\n",
    "\n",
    "                # Write back date list\n",
    "                #dts[...] = [n.encode(\"ascii\", \"ignore\") for n in dates]\n",
    "\n",
    "                # Manual garbage collect to prevent out of memory\n",
    "                [gc.collect() for x in range(3)]\n",
    "\n",
    "                parallel = True\n",
    "\n",
    "                \n",
    "                print(\"READY!\")\n",
    "            \n",
    "                if parallel:\n",
    "\n",
    "                    shared_arr = init_shared(self.chunks[0] * n)\n",
    "                    \n",
    "                    arr = tonumpyarray(shared_arr)\n",
    "\n",
    "                    arr.shape = (self.chunks[0], n)\n",
    "                    \n",
    "                    ysize = int(self.chunks[0]/self.rows)\n",
    "\n",
    "                    for b in range(0,dset.shape[0],self.chunks[0]):\n",
    "                        \n",
    "                        print(b)\n",
    "                        \n",
    "                        yoff = int(b/self.rows)\n",
    "                        \n",
    "                        for b1 in range(0,n,self.chunks[1]):\n",
    "                            \n",
    "                            arr[..., b1:b1+self.chunks[1]] = dset[b:b+self.chunks[0], b1:b1+self.chunks[1]]\n",
    "                            \n",
    "                        del b1\n",
    "\n",
    "                        with closing(mp.Pool(processes=8,initializer = init, initargs = (shared_arr,arr.shape,self.files))) as pool:\n",
    "                            \n",
    "                            res = pool.map(wload,[(0,yoff,2400,ysize,fix,dates_combined.index(self.dates[fix])) for fix in range(0,self.nfiles)])\n",
    "                            \n",
    "                        \n",
    "\n",
    "                        pool.close()\n",
    "                        pool.join()\n",
    "\n",
    "                        # sort\n",
    "\n",
    "                        arr[...] = arr[...,sort_ix]\n",
    "\n",
    "                        # write back\n",
    "                        for b1 in range(0,n,self.chunks[1]):\n",
    "                            dset[b:(b+self.chunks[0]), b1:(b1+self.chunks[1])] = arr[..., b1:(b1+self.chunks[1])]\n",
    "                else:\n",
    "                    \n",
    "                    t3 = time.time()\n",
    "                    \n",
    "                    file_handles = []\n",
    "                    \n",
    "                    arr = np.zeros((self.chunks[0],self.nfiles),dtype=self.datatype[1])\n",
    "                    \n",
    "                    print('Opening files ...')\n",
    "                    \n",
    "                    t1 = time.time()\n",
    "\n",
    "                    for ix,fl in enumerate(self.files):\n",
    "                        \n",
    "                        t2 = time.time()\n",
    "                        \n",
    "                        fl_o = gdal.Open(fl)\n",
    "\n",
    "                        val_sds = [x[0] for x in fl_o.GetSubDatasets() if self.paramdict[self.param] in x[0]][0]\n",
    "\n",
    "                        file_handles.append(gdal.Open(val_sds))\n",
    "\n",
    "                        fl_o = None\n",
    "                        \n",
    "                        if ix%100 == 0:\n",
    "                            print('File {} of {}. Elapsed {}.'.format(ix,self.nfiles,time.time()-t2))\n",
    "                            \n",
    "                    print(\"Done. Elapsed: {}\".format(time.time() - t1))\n",
    "\n",
    "                    ysize = self.chunks[0]//self.rows\n",
    "                    \n",
    "                    print('Writing data ...')\n",
    "                    \n",
    "                    bix = 0\n",
    "                    nblocks = dset.shape[0]//self.chunks[0]\n",
    "\n",
    "                    for b in range(0, dset.shape[0], self.chunks[0]):\n",
    "                        bix += 1\n",
    "                        t1 = time.time()\n",
    "\n",
    "                        yoff = b//self.rows\n",
    "\n",
    "                        for fix,f in enumerate(self.files):\n",
    "\n",
    "                            dset[b:b+self.chunks[0], dates_combined.index(self.dates[fix])] = file_handles[fix].ReadAsArray(xoff=0,yoff=yoff,xsize=self.rows,ysize=ysize).flatten()\n",
    "\n",
    "                        if bix%10==0:\n",
    "                            print(\"block {} of {}. Elapsed: {}\".format(bix,nblocks,time.time()-t1))\n",
    "                            \n",
    "                    for fix in range(len(file_handles)):\n",
    "                        file_handles[fix] = None\n",
    "\n",
    "                    del file_handles\n",
    "                    \n",
    "                    print(\"DONE! Total time: {}\".format(time.time()-t3))\n",
    "                \n",
    "\n",
    "        except:\n",
    "        \n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(**kwargs):\n",
    "    '''Initialize parameters for processing in workers.'''\n",
    "\n",
    "    params = dict()\n",
    "\n",
    "    for key, value in kwargs.items():\n",
    "        params[key] = value\n",
    "    return params\n",
    "\n",
    "\n",
    "def init(arr_,dim,files_):\n",
    "    \n",
    "    global arr\n",
    "    global files    \n",
    "    \n",
    "    files = files_\n",
    "    arr = tonumpyarray(arr_)\n",
    "    arr.shape = dim\n",
    "    \n",
    "def wload(ix):\n",
    "    \n",
    "    xoff, yoff, xsize, ysize, fix, aix = ix\n",
    "    try:\n",
    "        ds = gdal.Open(files[fix])\n",
    "        sds_o = gdal.Open(ds.GetSubDatasets()[0][0])\n",
    "    \n",
    "        arr[...,aix] =  sds_o.ReadAsArray(xoff,yoff,xsize,ysize).flatten()\n",
    "        ds = None\n",
    "        sds_o = None\n",
    "    except AttributeError:\n",
    "        print(files[fix])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fls = glob.glob('/data/h5py_test/*hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5 = MODISrawh5(fls,chunk=480**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.os.remove('/home/jovyan/VIM/MXD13A1.h17v07.006.VIM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = time.time()\n",
    "                    \n",
    "file_handles = []\n",
    "                    \n",
    "arr = np.zeros((h5.chunk,len(fls)),dtype='int16')\n",
    "                    \n",
    "print('Opening files ...')\n",
    "                    \n",
    "t1 = time.time()\n",
    "\n",
    "for ix,fl in enumerate(h5.files):\n",
    "    \n",
    "    t2 = time.time()\n",
    "    \n",
    "    fl_o = gdal.Open(fl)\n",
    "\n",
    "    val_sds = fl_o.GetSubDatasets()[0][0]\n",
    "\n",
    "    file_handles.append(gdal.Open(val_sds))\n",
    "\n",
    "    fl_o = None\n",
    "                        \n",
    "    if ix%100 == 0:\n",
    "        print('File {} of {}. Elapsed {}.'.format(ix,len(fls),time.time()-t2))\n",
    "                            \n",
    "print(\"Done. Elapsed: {}\".format(time.time() - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(fls)\n",
    "\n",
    "with h5py.File(h5.outname) as h5f:\n",
    "\n",
    "    dset = h5f.get(\"data\")\n",
    "    print(dset.chunks)\n",
    "    \n",
    "\n",
    "    ysize = h5.chunk//h5.rows\n",
    "                    \n",
    "    print('Writing data ...')\n",
    "        \n",
    "    ix = np.arange(len(fls))\n",
    "\n",
    "    bix = 0\n",
    "\n",
    "    nblocks = dset.shape[0]//h5.chunk\n",
    "\n",
    "    for b in range(0, dset.shape[0], dset.chunks[0]):\n",
    "        bix += 1\n",
    "        t1 = time.time()\n",
    "        yoff = b//h5.rows\n",
    "        \n",
    "        for b1 in range(0, n, dset.chunks[1]):\n",
    "            \n",
    "            arr[...,b1:b1+dset.chunks[1]] = dset[b:b+dset.chunks[0],b1:b1+dset.chunks[1]]\n",
    "        \n",
    "\n",
    "        for fix,f in enumerate(h5.files):\n",
    "            arr[...,h5.dates.index(h5.dates[fix])] = file_handles[fix].ReadAsArray(xoff=0,yoff=yoff,xsize=h5.rows,ysize=ysize).flatten()\n",
    "        \n",
    "        arr = arr[...,ix]\n",
    "        \n",
    "        for b1 in range(0, n, dset.chunks[1]):\n",
    "            \n",
    "            dset[b:b+dset.chunks[0],b1:b1+dset.chunks[1]] =  arr[...,b1:b1+dset.chunks[1]] \n",
    "        \n",
    "        if bix%10==0:\n",
    "            \n",
    "            print(\"block {} of {}. Elapsed: {}\".format(bix,nblocks,time.time()-t1))\n",
    "            \n",
    "        \n",
    "        print(\"block {} of {}. Elapsed: {}\".format(bix,nblocks,time.time()-t1))\n",
    "        if bix == 5:\n",
    "            break\n",
    "                            \n",
    "\n",
    "# for fix in range(len(file_handles)):\n",
    "#     file_handles[fix] = None\n",
    "\n",
    "# del file_handles\n",
    "                    \n",
    "print(\"DONE! Total time: {}\".format(time.time()-t3))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(100*25)/60"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "240**2 = 55.2 (33.1 * 100)\n",
    "480**2 = 23.75 (57 * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class handler:\n",
    "    def __init__(self,x):\n",
    "        self.n = []\n",
    "        for ii in range(x):\n",
    "            self.n.append(ii)\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = handler(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fix in range(len(file_handles)):\n",
    "    file_handles[fix] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del file_handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = gdal.Open(\"test.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'GetSubDatasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ea2285552075>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetSubDatasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'GetSubDatasets'"
     ]
    }
   ],
   "source": [
    "ds.GetSubDatasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
